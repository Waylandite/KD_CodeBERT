Tokenizer,Vocab Size,Num Hidden Layers,Hidden Size,Hidden Act,Hidden Dropout Prob,Intermediate Size,Num Attention Heads,Attention Probs Dropout Prob,Max Sequence Length,Position Embedding Type,Learning Rate,Batch Size,Accuracy
Unigram,13025,10,192,gelu_new,0.3,386,12,0.3,283,relative_key_query,0.001,16,"[0.5655197657393851, 0.6277452415812591, 0.6244509516837482, 0.5655197657393851, 0.6142020497803806, 0.6325036603221084, 0.5655197657393851, 0.5666178623718887, 0.636896046852123, 0.5904099560761347, 0.6215226939970717, 0.5655197657393851, 0.5655197657393851, 0.6218887262079063, 0.6233528550512445, 0.6145680819912153, 0.5728404099560761, 0.5655197657393851, 0.5980966325036603, 0.5951683748169839]"
WordPiece,44553,11,204,silu,0.4,949,4,0.4,497,relative_key_query,0.0001,8,"[0.0, 0.4415156507413509, 0.5116918844566712, 0.0, 0.4234135667396062, 0.45670995670995673, 0.0, 0.005042016806722689, 0.5216972034715526, 0.4024960998439937, 0.45758354755784064, 0.0, 0.0, 0.4701073071027082, 0.4693140794223827, 0.37949322333529756, 0.11254752851711027, 0.0, 0.38105975197294245, 0.38791208791208787]"
BPE,29357,6,70,gelu,0.2,2641,10,0.4,362,absolute,0.0001,8,"[0.0, 0.6368159203980099, 0.603085553997195, 0.0, 0.6108202443280978, 1.0, 0.0, 1.0, 0.6152796725784447, 0.56640625, 1.0, 0.0, 0.0, 0.6214953271028038, 0.665158371040724, 1.0, 0.6153846153846154, 0.0, 0.575809199318569, 0.5721925133689839]"
WordPiece,2261,2,148,silu,0.6,2489,4,0.1,382,relative_key_query,0.001,8,"[0.0, 0.33866891322662174, 0.4700926705981466, 0.0, 0.32603201347935973, 0.35551811288963775, 0.0, 0.002527379949452401, 0.45914069081718617, 0.32603201347935973, 0.37489469250210616, 0.0, 0.0, 0.3875315922493682, 0.38331929233361417, 0.2712721145745577, 0.062342038753159225, 0.0, 0.2847514743049705, 0.2973883740522325]"
